
# # Example:
# # spark.master                     spark://master:7077
# # spark.eventLog.enabled           true
# # spark.eventLog.dir               hdfs://namenode:8021/directory
# # spark.serializer                 org.apache.spark.serializer.KryoSerializer
# # spark.driver.memory              5g
# # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# # spark.hadoop.fs.s3a.endpoint http://minio.minio.svc.cluster.local:9000
# spark.hadoop.fs.s3a.endpoint http://minio:9000
# spark.hadoop.fs.s3a.access.key minio
# spark.hadoop.fs.s3a.secret.key miniominio
# # spark.hadoop.fs.s3a.path.style.access  true
# spark.hadoop.fs.s3a.impl   org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3.impl   org.apache.hadoop.fs.s3a.S3AFileSystem

# AWS S3 configuration

# spark.hadoop.fs.s3.impl                                        org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.endpoint                                   https://s3.amazonaws.com
# spark.hadoop.fs.s3a.aws.credentials.provider                   org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,com.amazonaws.auth.EnvironmentVariableCredentialsProvider,com.amazonaws.auth.InstanceProfileCredentialsProvider
# spark.hadoop.mapreduce.fileoutputcommiter.algorithm.version    2

# Delta Lake configuration

spark.sql.extensions                                           io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog                                org.apache.spark.sql.delta.catalog.DeltaCatalog

# You can use environment variables too o set in conf below
# spark.hadoop.fs.s3a.access.key ******
# spark.hadoop.fs.s3a.secret.key ******

# Minio
spark.hadoop.fs.s3.impl                                        org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.impl                                       org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint                                   http://minio:9000
spark.hadoop.fs.s3a.aws.credentials.provider                   org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,com.amazonaws.auth.EnvironmentVariableCredentialsProvider,com.amazonaws.auth.InstanceProfileCredentialsProvider
spark.hadoop.fs.s3a.access.key                                 minio
spark.hadoop.fs.s3a.secret.key                                 miniominio
spark.hadoop.fs.s3a.path.style.access                          true
# spark.hadoop.fs.s3a.path.style.access  true

# Google Cloud Storage configuration
# More in:
#  https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md
#  https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md

# The AbstractFileSystem for 'gs:' URIs
# spark.hadoop.fs.AbstractFileSystem.gs.impl                     com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS

# Optional. Google Cloud Project ID with access to GCS buckets.
# Required only for list buckets and create bucket operations.
# spark.hadoop.fs.gs.project.id

# Whether to use a service account for GCS authorization. Setting this
# property to `false` will disable use of service accounts for authentication.
# spark.hadoop.google.cloud.auth.service.account.enable        true

# The JSON keyfile of the service account used for GCS
# access when google.cloud.auth.service.account.enable is true.
# spark.hadoop.google.cloud.auth.service.account.json.keyfile    /path/to/keyfile